{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ab2ec8-c440-44ac-9e41-f2c568bafbd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Fine Tune Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b013490f-c914-4f1e-8e6a-2d1da9ddc4e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whitleyo/anaconda3/envs/watspeed_data_gr_proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_164805/579269314.py:13: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel # FastLanguageModel for LLMs\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a030fd-70fd-4238-ab62-18efc30d1925",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37edda7-3070-44a4-8f99-dbe9ec902718",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_path = '../'\n",
    "s3_bucket = \"watspeed-data-gr-project\"\n",
    "s3_prefix = \"models\"\n",
    "use_s3 = True\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "mongo_db_name = \"biorxiv\"\n",
    "mongo_db_collection = \"abstracts\"\n",
    "local_model_path = \"models\"\n",
    "base_model_name = \"unsloth/Llama-3.2-3B\" \n",
    "use_adapted_model = False\n",
    "adapter_path = None # path is relative to local_model_path or s3_prefix\"\n",
    "use_time_series_split = False\n",
    "test_size = 0.10\n",
    "report_to = \"tensorboard\" # report to tensorboard\n",
    "disable_tqdm = False\n",
    "eval_steps=None\n",
    "eval_strategy=\"epoch\"\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False. Works for llama 8b but not 3.2-1b\n",
    "per_device_train_batch_size=8\n",
    "gradient_accumulation_steps=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b42e5-e04c-4a47-8594-1494076bf66a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(app_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7342ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env â€” assuming local environment\n"
     ]
    }
   ],
   "source": [
    "from utils.aws import get_boto3_client\n",
    "if use_s3:\n",
    "    s3 = get_boto3_client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cfb640",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(local_model_path):\n",
    "    os.makedirs(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd83a-fb66-47eb-9505-558cdecf35ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f3b95b-f737-46b9-8180-e6dbd49f6e62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup\n",
      "2025-08-11 23:41:17.324168\n",
      "==((====))==  Unsloth 2025.8.4: Fast Llama patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "Unsloth 2025.8.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "## Model Setup\n",
    "print('Model Setup')\n",
    "print(datetime.now())\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "\n",
    "\n",
    "\n",
    "if use_adapted_model:\n",
    "    # if use_s3, download the adapted model from S3 from specified, bucket, prefix and path\n",
    "    assert adapter_path is not None, \"Adapter path must be specified when using adapted model.\"\n",
    "    if use_s3:\n",
    "        # assert s3 handler exists\n",
    "        assert s3 is not None, \"S3 client is not initialized.\"\n",
    "        s3_model_path = f\"{s3_prefix}/{adapter_path}\"\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "        # Wipe local directory if it exists\n",
    "        # if os.path.exists(full_model_local_path):\n",
    "        #     os.rmdir(full_model_local_path)\n",
    "        os.makedirs(full_local_model_path, exist_ok=True)\n",
    "        # List all objects under the prefix\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_model_path):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                if key.endswith('/'):  # Skip folders\n",
    "                    continue\n",
    "                # Determine local file path\n",
    "                rel_path = os.path.basename(key)\n",
    "                local_path = os.path.join(full_local_model_path, rel_path)\n",
    "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    \n",
    "                print(f\"Downloading {key} to {local_path}\")\n",
    "                s3.download_file(s3_bucket, key, local_path)\n",
    "    else:\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = full_local_model_path,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit\n",
    "        #\n",
    "    )\n",
    "else:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = base_model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "                model,\n",
    "                r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "                target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                  \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "                # layers_to_transform=[num_layers - 1],\n",
    "                lora_alpha = 16,\n",
    "                lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "                bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "                # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "                use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "                random_state = 3407,\n",
    "                use_rslora = False,  # We support rank stabilized LoRA\n",
    "                loftq_config = None, # And LoftQ\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e935bda-4e1d-4805-b502-93e9e9c21a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbaa83-0bab-4d79-a47c-6020200e7571",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f63d13c-06ef-485a-a92c-7938ef556e8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.pytorch_dataset import BioRxivDataset\n",
    "# dataset = load_dataset(\"your_dataset_name\", split=\"train\")\n",
    "dataset = BioRxivDataset(mongo_uri=mongo_uri,\n",
    "                         db_name=mongo_db_name,\n",
    "                         collection_name=mongo_db_collection,\n",
    "                         )\n",
    "# dataset.map(partial(tokenize_with_eos, tokenizer=tokenizer, max_length=max_seq_length))\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=test_size, \n",
    "                                random_state=42, \n",
    "                                use_time_series_split=use_time_series_split\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f103a34a-0d70-4c84-a5d5-90cd033d3912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.pytorch_dataset.BioRxivDataset at 0x73e8dc201e20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e30a71-577c-4d10-a347-701235b59a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ab83a5-7d2d-4976-bfc7-0a38764c76c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4349"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc7b48d-3486-4bda-8542-9c275e8633d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '689835383c834e4e5e1097b7',\n",
       "  'doi': '10.1101/2024.07.24.604968',\n",
       "  'text': 'Phylogenomics has emerged as a transformative approach in systematics, conservation biology, and biomedicine, enabling the inference of evolutionary relationships by leveraging hundreds to thousands of genes from genomic or transcriptomic data. However, acquiring high-quality genomes and transcriptomes necessitates samples with intact DNA and RNA, substantial sequencing investments, and extensive bioinformatic processing, such as genome/transcriptome assembly and annotation. This challenge is particularly pronounced for rare or difficult-to-collect species, such as those inhabiting the deep sea, where only fragmented DNA reads are often available due to environmental degradation or suboptimal preservation conditions. To address these limitations, we introduce VEHoP (Versatile, Easy-to-use Homology-based Phylogenomic pipeline), a tool designed to infer protein-coding regions from diverse inputs, including raw reads (short and long), draft genomes, transcriptomes, and annotated genomes. VEHoP automates the generation of orthologous sequence alignments, concatenated matrices, and phylogenetic trees, streamlining phylogenomic analyses for researchers across disciplines. The tool aims to (1) expand taxonomic sampling by accommodating a wide range of input data types and (2) simplify phylogenomic workflows, making them accessible to researchers with varying levels of bioinformatic expertise. We evaluated VEHoPs performance using datasets from oysters, catfish, and insects, demonstrating its ability to produce robust phylogenetic trees with strong bootstrap support, outperforming assembly-free methods. Additionally, we applied VEHoP to reconstruct the phylogeny of the enigmatic deep-sea gastropod order Neomphalida, successfully resolving a well-supported phylogenetic backbone for this poorly understood group. VEHoP is freely available on GitHub (https://github.com/ylify/VEHoP), with dependencies easily installable via Bioconda.',\n",
       "  'date': '2025-01-21'},\n",
       " {'_id': '689838153c834e4e5e10bb33',\n",
       "  'doi': '10.1101/2025.05.28.656557',\n",
       "  'text': 'BackgroundIn mild cognitive impairment and dementia due to Alzheimers disease (AD), postmortem and in vivo neuroimaging studies have demonstrated significant neuronal loss in the basal forebrain cholinergic system (BFCS), which provides the primary cholinergic input to the cerebral cortex. Within this region, atrophy is most prominent in the nucleus basalis of Meynert (nbM), a group of posteriorly clustered magnocellular neurons in the BFCS. However, less is known surrounding the relationship between amyloid deposition, BFCS atrophy, and medial temporal lobe (MTL) volume loss in the preclinical stages of AD. The current study investigates the relationship between sub-structural BFCS volume and cortical A{beta} burden in cognitively unimpaired middle-aged individuals at varying genetic risk for AD.\\n\\nMethodsCognitively unimpaired participants aged 50-65 with a first-degree family history for AD were genetically screened to select three groups: APOE genotype {varepsilon}4{varepsilon}4 (n=15), {varepsilon}3{varepsilon}4 (n=15), and {varepsilon}3{varepsilon}3 (n=15), matched for age and sex. Participants underwent imaging with [11C]PiB PET and structural 3T MRI. Distribution volumes ratios (DVR) with a whole cerebellum reference region were calculated for [11C]PiB PET analyses. BFCS sub-structural volumes were obtained from the SPM8 Anatomy Toolbox (Cholinergic nuclei [Ch] 1-3, Ch4). MTL subregional volumes (entorhinal cortex, hippocampus, amygdala, parahippocampal gyrus) were extracted using Freesurfer.\\n\\nResultsBFCS amyloid burden was highest among APOE {varepsilon}4 homozygotes (Ch1-3, F(2, 42)=3.26, P=0.048; Ch4, F(2, 42)=3.82, P= 0.03). Ch4 (nbM), but not Ch1-3 volume, was found to be inversely associated with global A{beta} burden (Pearson r=-0.40, P=0.007). MTL subregional volumes were not associated with global A{beta} burden in the pooled sample. Exploratory analyses in groups stratified by amyloid positivity demonstrated reduced Ch4 volume (P=0.032) and significant inverse associations between Ch4 volume and amyloid burden (Pearson r = -0.70, P=0.02) in A{beta}+ participants.\\n\\nConclusionsWe observed nbM (Ch4), but not MTL volume, to be significantly inversely associated with cortical amyloid burden in cognitively unimpaired, A{beta}+, middle-aged adults at varying genetic risk for AD. These findings provide further in vivo evidence suggesting that nbM atrophy is an early structural correlate of AD pathogenesis, potentially preceding MTL atrophy.',\n",
       "  'date': '2025-06-01'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_dict()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669842ce-b7da-499c-ae09-d69c8ecd0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos(example):\n",
    "    eos_token = tokenizer.eos_token\n",
    "    if eos_token is None:\n",
    "        raise ValueError(\"Tokenizer does not define an EOS token.\")\n",
    "    \n",
    "    text = example.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        return {\"text\": \"\"}\n",
    "    \n",
    "    return {\"text\": text + eos_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd01aaed-3d5b-48ed-9517-d6ab53f09305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39138/39138 [00:00<00:00, 496749.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4349/4349 [00:00<00:00, 469875.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "train_hf_dataset = []\n",
    "eval_hf_dataset = []\n",
    "print('converting train data')\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    item = train_dataset[i]\n",
    "    if \"text\" in item.keys():\n",
    "        train_hf_dataset.append(add_eos(item))\n",
    "    else:\n",
    "        print(\"skipping for index {} in train dataset\".format(i))\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "    item = eval_dataset[i]\n",
    "    if \"text\" in item.keys():\n",
    "        eval_hf_dataset.append(add_eos(item))\n",
    "    else:\n",
    "        print(\"skipping for index {} in eval dataset\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeac21c4-5dde-4efd-b616-c5205bdce406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = Dataset.from_list(train_hf_dataset)\n",
    "eval_hf_dataset = Dataset.from_list(eval_hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d130e0c-edec-4233-af1b-0236fc1d83aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e1c89d-7b0f-4f6d-88bd-a0d75eb3558a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39138/39138 [00:04<00:00, 9317.64 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4349/4349 [00:00<00:00, 9567.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_hf_dataset,\n",
    "    eval_dataset = eval_hf_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = per_device_train_batch_size,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        # max_steps = 5,\n",
    "        eval_steps=eval_steps,\n",
    "        eval_strategy = eval_strategy,\n",
    "        learning_rate = 1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = report_to, # Use this for WandB etc\n",
    "        logging_dir = \"logs\",\n",
    "        disable_tqdm = disable_tqdm\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81da89",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ca0254-3c0d-4cf6-a7fc-79856f8618d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4060 Laptop GPU. Max memory = 7.996 GB.\n",
      "2.953 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc104590-96f3-42a3-b5e2-7f271f33ed57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday, August 11, 2025 at 11:41 PM'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f6ca5e-3972-45b7-96cd-5f98eeceb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 39,138 | Num Epochs = 2 | Total steps = 1,224\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 12,156,928 of 3,224,906,752 (0.38% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1224/1224 9:25:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.251900</td>\n",
       "      <td>2.198221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.130800</td>\n",
       "      <td>2.192798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5ca626-2745-43b6-bea6-14b0a92859d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuesday, August 12, 2025 at 09:07 AM'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa1a129-39d2-4c0c-a096-c2f9097ddde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33948.032 seconds used for training.\n",
      "565.8 minutes used for training.\n",
      "Peak reserved memory = 5.684 GB.\n",
      "Peak reserved memory for training = 2.731 GB.\n",
      "Peak reserved memory % of max memory = 71.086 %.\n",
      "Peak reserved memory for training % of max memory = 34.155 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210652d8",
   "metadata": {},
   "source": [
    "## Save Lora Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84cdba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA Weights...\n",
      "Uploading LoRA Weight Files to S3...\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/special_tokens_map.json\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/tokenizer_config.json\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/training_args.bin\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/README.md\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/adapter_model.safetensors\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/tokenizer.json\n",
      "models/unsloth_Llama-3.2-3B_20250812_090718/lora_weights/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Save LoRA Weights locally and to S3 if required\n",
    "print(\"Saving LoRA Weights...\")\n",
    "\n",
    "base_model_folder = base_model_name.replace(\"/\", \"_\") + \"_{}\".format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "model_subdir = os.path.join(local_model_path, base_model_folder)\n",
    "if not os.path.exists(model_subdir):\n",
    "    os.makedirs(model_subdir)\n",
    "lora_weights_path = os.path.join(model_subdir, \"lora_weights\")\n",
    "if not os.path.exists(lora_weights_path):\n",
    "    os.makedirs(lora_weights_path)\n",
    "trainer.save_model(lora_weights_path)\n",
    "tokenizer.save_pretrained(lora_weights_path)\n",
    "if use_s3:\n",
    "    print(\"Uploading LoRA Weight Files to S3...\")\n",
    "    for fname in os.listdir(lora_weights_path):\n",
    "        fpath = os.path.join(lora_weights_path, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            print(\"{}\".format(fpath))\n",
    "            s3.upload_file(\n",
    "                Filename=os.path.join(lora_weights_path, fname),\n",
    "                Bucket=s3_bucket,\n",
    "                Key=os.path.join(s3_prefix, base_model_folder, \"lora_weights\", fname)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99f54aa5-d14c-4ee5-934a-b4af895ab801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuesday, August 12, 2025 at 09:07 AM'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watspeed_data_gr_proj",
   "language": "python",
   "name": "watspeed_data_gr_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
