{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ab2ec8-c440-44ac-9e41-f2c568bafbd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Fine Tune Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b013490f-c914-4f1e-8e6a-2d1da9ddc4e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whitleyo/anaconda3/envs/watspeed_data_gr_proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_135255/579269314.py:13: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel # FastLanguageModel for LLMs\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a030fd-70fd-4238-ab62-18efc30d1925",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37edda7-3070-44a4-8f99-dbe9ec902718",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_path = '../'\n",
    "s3_bucket = \"watspeed-data-gr-project\"\n",
    "s3_prefix = \"models\"\n",
    "use_s3 = True\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "mongo_db_name = \"biorxiv\"\n",
    "mongo_db_collection = \"abstracts\"\n",
    "local_model_path = \"models\"\n",
    "base_model_name = \"unsloth/Llama-3.2-1B\"\n",
    "use_adapted_model = False\n",
    "adapter_path = None # path is relative to local_model_path or s3_prefix\"\n",
    "use_time_series_split = False\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b42e5-e04c-4a47-8594-1494076bf66a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(app_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7342ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env â€” assuming local environment\n"
     ]
    }
   ],
   "source": [
    "from utils.aws import get_boto3_client\n",
    "if use_s3:\n",
    "    s3 = get_boto3_client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cfb640",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(local_model_path):\n",
    "    os.makedirs(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd83a-fb66-47eb-9505-558cdecf35ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f3b95b-f737-46b9-8180-e6dbd49f6e62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Setup\n",
      "2025-08-11 21:22:20.862443\n",
      "==((====))==  Unsloth 2025.8.4: Fast Llama patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "## Model Setup\n",
    "print('Model Setup')\n",
    "print(datetime.now())\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "if use_adapted_model:\n",
    "    # if use_s3, download the adapted model from S3 from specified, bucket, prefix and path\n",
    "    assert adapter_path is not None, \"Adapter path must be specified when using adapted model.\"\n",
    "    if use_s3:\n",
    "        # assert s3 handler exists\n",
    "        assert s3 is not None, \"S3 client is not initialized.\"\n",
    "        s3_model_path = f\"{s3_prefix}/{adapter_path}\"\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "        # Wipe local directory if it exists\n",
    "        # if os.path.exists(full_model_local_path):\n",
    "        #     os.rmdir(full_model_local_path)\n",
    "        os.makedirs(full_local_model_path, exist_ok=True)\n",
    "        # List all objects under the prefix\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_model_path):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                if key.endswith('/'):  # Skip folders\n",
    "                    continue\n",
    "                # Determine local file path\n",
    "                rel_path = os.path.basename(key)\n",
    "                local_path = os.path.join(full_local_model_path, rel_path)\n",
    "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    \n",
    "                print(f\"Downloading {key} to {local_path}\")\n",
    "                s3.download_file(s3_bucket, key, local_path)\n",
    "    else:\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = full_local_model_path,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit\n",
    "        #\n",
    "    )\n",
    "else:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = base_model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "                model,\n",
    "                r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "                target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                  \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "                lora_alpha = 16,\n",
    "                lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "                bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "                # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "                use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "                random_state = 3407,\n",
    "                use_rslora = False,  # We support rank stabilized LoRA\n",
    "                loftq_config = None, # And LoftQ\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e935bda-4e1d-4805-b502-93e9e9c21a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbaa83-0bab-4d79-a47c-6020200e7571",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f63d13c-06ef-485a-a92c-7938ef556e8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.pytorch_dataset import BioRxivDataset\n",
    "# dataset = load_dataset(\"your_dataset_name\", split=\"train\")\n",
    "dataset = BioRxivDataset(mongo_uri=mongo_uri,\n",
    "                         db_name=mongo_db_name,\n",
    "                         collection_name=mongo_db_collection,\n",
    "                         )\n",
    "# dataset.map(partial(tokenize_with_eos, tokenizer=tokenizer, max_length=max_seq_length))\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=test_size, \n",
    "                                random_state=42, \n",
    "                                use_time_series_split=use_time_series_split\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f103a34a-0d70-4c84-a5d5-90cd033d3912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.pytorch_dataset.BioRxivDataset at 0x7981f42826f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4e30a71-577c-4d10-a347-701235b59a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34789"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ab83a5-7d2d-4976-bfc7-0a38764c76c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8698"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc7b48d-3486-4bda-8542-9c275e8633d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': '68982f433c834e4e5e104618',\n",
       "  'doi': '10.1101/2024.01.25.577194',\n",
       "  'text': 'In recent years, a vast number of novel antiphage defense mechanisms were uncovered. To facilitate the exploration of mechanistic, ecological, and evolutionary aspects related to antiphage defense systems, we released DefenseFinder in 2021 (Tesson et al., 2022). DefenseFinder is a bioinformatic program designed for the systematic identification of known antiphage defense mechanisms. The initial release of DefenseFinder v1.0.0 included 60 systems. Over the past three years, the number of antiphage systems incorporated into DefenseFinder has grown to 152. The increasing number of known systems makes it a challenge to enter the field and makes the interpretation of detections of antiphage systems difficult. Moreover, the rapid development of sequence-based predictions of structures offers novel possibilities of analysis and should be easily available. To overcome these challenges, we present a hub of resources on defense systems, including: 1) an updated version of DefenseFinder with a web-service search function, 2) a community-curated repository of knowledge on the systems, and 3) precomputed databases, which include annotations done on RefSeq genomes and structure predictions generated by AlphaFold. These pages can be freely accessed for users as a starting point on their journey to better understand a given system. We anticipate that these resources will foster the use of bioinformatics in the study of antiphage systems and will serve the community of researchers who study antiphage systems. This resource is available at: https://defensefinder.mdmlab.fr.',\n",
       "  'date': '2024-04-12'},\n",
       " {'_id': '689830393c834e4e5e1054bc',\n",
       "  'doi': '10.1101/2020.12.30.424670',\n",
       "  'text': 'Withdrawal StatementThe authors have withdrawn their manuscript owing to the following reason: Unfortunately, the preparation of the recombinant serine protease-like protein B (SplB) used in this study was found to contain impurities, so that the effects presented cannot be unambiguously attributed to the protease SplB. We therefore retract the manuscript from bioRxiv. We apologize for any inconvenience this may have caused. Therefore, the authors do not wish this work to be cited as reference for the project. We want to clarify that the issue was identified in the preparation of the SplB protein and does not reflect on the contributions of individual authors. If you have any questions, please contact the corresponding author.',\n",
       "  'date': '2024-05-29'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_dict()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669842ce-b7da-499c-ae09-d69c8ecd0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos(example):\n",
    "    eos_token = tokenizer.eos_token\n",
    "    if eos_token is None:\n",
    "        raise ValueError(\"Tokenizer does not define an EOS token.\")\n",
    "    \n",
    "    text = example.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        return {\"text\": \"\"}\n",
    "    \n",
    "    return {\"text\": text + eos_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd01aaed-3d5b-48ed-9517-d6ab53f09305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34789/34789 [00:00<00:00, 529812.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8698/8698 [00:00<00:00, 438734.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "train_hf_dataset = []\n",
    "eval_hf_dataset = []\n",
    "print('converting train data')\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    item = train_dataset[i]\n",
    "    if \"text\" in item.keys():\n",
    "        train_hf_dataset.append(add_eos(item))\n",
    "    else:\n",
    "        print(\"skipping for index {} in train dataset\".format(i))\n",
    "for i in tqdm(range(len(eval_dataset))):\n",
    "    item = eval_dataset[i]\n",
    "    if \"text\" in item.keys():\n",
    "        eval_hf_dataset.append(add_eos(item))\n",
    "    else:\n",
    "        print(\"skipping for index {} in eval dataset\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeac21c4-5dde-4efd-b616-c5205bdce406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf_dataset = Dataset.from_list(train_hf_dataset)\n",
    "eval_hf_dataset = Dataset.from_list(eval_hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d130e0c-edec-4233-af1b-0236fc1d83aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e1c89d-7b0f-4f6d-88bd-a0d75eb3558a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34789/34789 [00:03<00:00, 9882.82 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8698/8698 [00:00<00:00, 9582.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_hf_dataset,\n",
    "    eval_dataset = eval_hf_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81da89",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ca0254-3c0d-4cf6-a7fc-79856f8618d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4060 Laptop GPU. Max memory = 7.996 GB.\n",
      "2.41 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc104590-96f3-42a3-b5e2-7f271f33ed57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday, August 11, 2025 at 09:22 PM'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f6ca5e-3972-45b7-96cd-5f98eeceb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 34,789 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.206900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.471600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5ca626-2745-43b6-bea6-14b0a92859d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday, August 11, 2025 at 09:22 PM'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa1a129-39d2-4c0c-a096-c2f9097ddde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.514 seconds used for training.\n",
      "0.19 minutes used for training.\n",
      "Peak reserved memory = 3.236 GB.\n",
      "Peak reserved memory for training = 0.826 GB.\n",
      "Peak reserved memory % of max memory = 40.47 %.\n",
      "Peak reserved memory for training % of max memory = 10.33 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210652d8",
   "metadata": {},
   "source": [
    "## Save Lora Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84cdba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA Weights...\n",
      "Uploading LoRA Weight Files to S3...\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/special_tokens_map.json\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/tokenizer_config.json\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/training_args.bin\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/README.md\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/adapter_model.safetensors\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/tokenizer.json\n",
      "models/unsloth_Llama-3.2-1B_20250811_212242/lora_weights/adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Save LoRA Weights locally and to S3 if required\n",
    "print(\"Saving LoRA Weights...\")\n",
    "\n",
    "base_model_folder = base_model_name.replace(\"/\", \"_\") + \"_{}\".format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "model_subdir = os.path.join(local_model_path, base_model_folder)\n",
    "if not os.path.exists(model_subdir):\n",
    "    os.makedirs(model_subdir)\n",
    "lora_weights_path = os.path.join(model_subdir, \"lora_weights\")\n",
    "if not os.path.exists(lora_weights_path):\n",
    "    os.makedirs(lora_weights_path)\n",
    "trainer.save_model(lora_weights_path)\n",
    "tokenizer.save_pretrained(lora_weights_path)\n",
    "if use_s3:\n",
    "    print(\"Uploading LoRA Weight Files to S3...\")\n",
    "    for fname in os.listdir(lora_weights_path):\n",
    "        fpath = os.path.join(lora_weights_path, fname)\n",
    "        if os.path.isfile(fpath):\n",
    "            print(\"{}\".format(fpath))\n",
    "            s3.upload_file(\n",
    "                Filename=os.path.join(lora_weights_path, fname),\n",
    "                Bucket=s3_bucket,\n",
    "                Key=os.path.join(s3_prefix, base_model_folder, \"lora_weights\", fname)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99f54aa5-d14c-4ee5-934a-b4af895ab801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday, August 11, 2025 at 09:22 PM'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%A, %B %d, %Y at %I:%M %p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watspeed_data_gr_proj",
   "language": "python",
   "name": "watspeed_data_gr_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
