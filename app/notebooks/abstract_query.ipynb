{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af9e8b6-f53e-480b-8e6f-9fae8d08e70c",
   "metadata": {},
   "source": [
    "# Abstract Query Tool\n",
    "\n",
    "Here, we take your abstract text, find the top K most similar abstracts based on SentenceTransformers embeddings,\n",
    "and generate a summary of the literature using a huggingface LLM (by default LLama 3.2 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e747b934e7a9",
   "metadata": {},
   "source": [
    "## Step 0 — Configuration\n",
    "Set MongoDB connection info, base model, LoRA weights path, AWS region, and generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:22:41.457375Z",
     "start_time": "2025-08-14T23:22:41.436405Z"
    }
   },
   "outputs": [],
   "source": [
    "app_path = '../'\n",
    "use_adapted_model = False\n",
    "use_s3 = True\n",
    "s3_bucket = \"watspeed-data-gr-project\"\n",
    "s3_prefix = \"models\"\n",
    "local_model_path = \"models\"\n",
    "base_model_name = \"unsloth/Meta-Llama-3.1-8B\" \n",
    "adapter_path = None #'unsloth_Llama-3.2-3B_20250812_090718/lora_weights' # path is relative to local_model_path or s3_prefix\"\n",
    "\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "mongo_db_name = \"biorxiv\"\n",
    "mongo_db_collection = \"abstracts\"\n",
    "\n",
    "\n",
    "start_date = '2025-07-01'\n",
    "end_date = None\n",
    "\n",
    "top_k           = 5\n",
    "model_max_length = 2048\n",
    "max_new_tokens = 256\n",
    "temperature     = 0.5\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "abstract_text = \"\"\"Glioblastomas harbor diverse cell populations, including rare glioblastoma stem cells (GSCs)\n",
    "                that drive tumorigenesis. To characterize functional diversity within this population, we performed \n",
    "                single-cell RNA sequencing on >69,000 GSCs cultured from the tumors of 26 patients. We observed a high \n",
    "                degree of inter- and intra-GSC transcriptional heterogeneity that could not be fully explained by DNA \n",
    "                somatic alterations. Instead, we found that GSCs mapped along a transcriptional gradient spanning two \n",
    "                cellular states reminiscent of normal neural development and inflammatory wound response. \n",
    "                Genome-wide CRISPR–Cas9 dropout screens independently recapitulated this observation, with each state \n",
    "                characterized by unique essential genes. Further single-cell RNA sequencing of >56,000 malignant cells \n",
    "                from primary tumors found that the majority organize along an orthogonal astrocyte maturation gradient \n",
    "                yet retain expression of founder GSC transcriptional programs. We propose that glioblastomas grow out \n",
    "                of a fundamental GSC-based neural wound response transcriptional program, which is a promising target \n",
    "                for new therapy development.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f0ac20-484c-4b2d-b0f2-9a144b9e0236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whitleyo/anaconda3/envs/watspeed_data_gr_proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_163107/1560833987.py:18: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel # FastLanguageModel for LLMs\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fdca1925cc325",
   "metadata": {},
   "source": [
    "## Step 1 — Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297512d0-c11e-422d-aa11-8d78fb27a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(app_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f334849-3c15-4e9b-bb6f-9930413fe571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env — assuming local environment\n"
     ]
    }
   ],
   "source": [
    "from utils.aws import get_boto3_client\n",
    "if use_s3:\n",
    "    s3 = get_boto3_client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adf173-19e0-42a6-8bd7-298784642ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632031d3-0c2e-49e0-a805-7bd4b546b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.4: Fast Llama patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
     ]
    }
   ],
   "source": [
    "if use_adapted_model:\n",
    "    # if use_s3, download the adapted model from S3 from specified, bucket, prefix and path\n",
    "    assert adapter_path is not None, \"Adapter path must be specified when using adapted model.\"\n",
    "    if use_s3:\n",
    "        # assert s3 handler exists\n",
    "        assert s3 is not None, \"S3 client is not initialized.\"\n",
    "        s3_model_path = f\"{s3_prefix}/{adapter_path}\"\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "        # Wipe local directory if it exists\n",
    "        # if os.path.exists(full_model_local_path):\n",
    "        #     os.rmdir(full_model_local_path)\n",
    "        os.makedirs(full_local_model_path, exist_ok=True)\n",
    "        # List all objects under the prefix\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_model_path):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                if key.endswith('/'):  # Skip folders\n",
    "                    continue\n",
    "                # Determine local file path\n",
    "                rel_path = os.path.basename(key)\n",
    "                local_path = os.path.join(full_local_model_path, rel_path)\n",
    "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    \n",
    "                print(f\"Downloading {key} to {local_path}\")\n",
    "                s3.download_file(s3_bucket, key, local_path)\n",
    "    else:\n",
    "        full_local_model_path = os.path.join(local_model_path, adapter_path)\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = full_local_model_path,\n",
    "        max_seq_length = model_max_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit\n",
    "        #\n",
    "    )\n",
    "else:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = base_model_name,\n",
    "        max_seq_length = model_max_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    # num_layers = model.config.num_hidden_layers\n",
    "    # model = FastLanguageModel.get_peft_model(\n",
    "    #             model,\n",
    "    #             r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    #             target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    #                               \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    #             # layers_to_transform=[num_layers - 1],\n",
    "    #             lora_alpha = 16,\n",
    "    #             lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    #             bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    #             # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    #             use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    #             random_state = 3407,\n",
    "    #             use_rslora = False,  # We support rank stabilized LoRA\n",
    "    #             loftq_config = None, # And LoftQ\n",
    "    #         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5f298d-72e7-49b7-827f-3f581035e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT adapters: None\n",
      "CUDA available: True | device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    print(\"PEFT adapters:\", getattr(model, \"peft_config\", None))\n",
    "except Exception as e:\n",
    "    print(\"PEFT not attached?\", e)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available(), \"| device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584f520191c7a5b",
   "metadata": {},
   "source": [
    "## Step 2 — Load Corpus given specified date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141971077fdd007d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:25:28.232109Z",
     "start_time": "2025-08-14T23:23:57.648002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 2025-07-01\n",
      "End Date: 2025-08-14\n",
      "Loaded 3030 abstracts from Mongo.\n"
     ]
    }
   ],
   "source": [
    "# --- Mongo → load docs (light filter) ---\n",
    "col = MongoClient(mongo_uri)[mongo_db_name][mongo_db_collection]\n",
    "if start_date is None:\n",
    "    min_doc = col.find_one({\"abstract\": {\"$ne\": \"\"}}, sort=[(\"date\", 1)])\n",
    "    start_date = min_doc[\"date\"] if min_doc else None\n",
    "print(\"Start Date: {}\".format(start_date))\n",
    "if end_date is None:\n",
    "    max_doc = col.find_one({\"abstract\": {\"$ne\": \"\"}}, sort=[(\"date\", -1)])\n",
    "    end_date = max_doc[\"date\"] if max_doc else None\n",
    "print(\"End Date: {}\".format(end_date))\n",
    "# Step 2: Build the query with date range\n",
    "query = {\n",
    "    \"abstract\": {\"$ne\": \"\"},\n",
    "    \"date\": {\"$gte\": start_date, \"$lte\": end_date}\n",
    "}\n",
    "docs = list(col.find(query, {\"_id\": 1, \"title\": 1, \"abstract\": 1}))\n",
    "assert docs, \"No docs found in Mongo. Check DB/collection.\"\n",
    "print(f\"Loaded {len(docs)} abstracts from Mongo.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f12cef-1ad4-43d2-bb09-5275d553b8f8",
   "metadata": {},
   "source": [
    "# STEP3: Calculate Embeddings and determine top K abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a773b574-ec17-4aa7-a0b9-9e46dfa17b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████| 95/95 [00:04<00:00, 20.10it/s]\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# Assuming 'docs' is your list of MongoDB documents\n",
    "abstracts = [doc['abstract'] for doc in docs if 'abstract' in doc]\n",
    "# Generate embeddings\n",
    "embeddings = embedder.encode(abstracts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b638ef09-82b9-468d-970b-cba2fd69bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedder.encode([abstract_text])\n",
    "similarities = cosine_similarity(query_embedding, embeddings)[0]  # shape: (num_abstracts,)\n",
    "\n",
    "# Step 3: Get top k indexes\n",
    "top_k = 5  # or whatever number you want\n",
    "top_k_indices = np.argsort(similarities)[-top_k:][::-1]  # sorted in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bb9279c6fbca8",
   "metadata": {},
   "source": [
    "## Step 3 — Use Llama for Summarization\n",
    "Cleans abstracts, truncates them to safe length, rebuilds embeddings, constructs summarization prompt, and generates output with retry logic to avoid boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afbd08be-c519-4277-bbf6-702be1b59144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:02:54.117576Z",
     "start_time": "2025-08-14T16:48:15.560441Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_literature(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    query_abstract,\n",
    "    top_k_abstracts,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0\n",
    "):\n",
    "    current_summary = \"\"\n",
    "\n",
    "    for i, doc in enumerate(top_k_abstracts):\n",
    "        abstract_i = doc.get(\"abstract\", \"\").strip()\n",
    "        if not abstract_i:\n",
    "            continue\n",
    "\n",
    "        # Build prompt as a simple text completion task for a base model\n",
    "        # The prompt is a single string with no special chat tokens.\n",
    "        prompt = (\n",
    "            \"You are a scientific summarization assistant.\\n\\n\"\n",
    "            \"Your task is to update a literature summary based on a new abstract and a query abstract.\\n\\n\"\n",
    "            f\"Inputs:\\n\"\n",
    "            f\"- Current summary: '{current_summary}'\\n\"\n",
    "            f\"- Query abstract: '{query_abstract}'\\n\"\n",
    "            f\"- New abstract (ranked {i+1}th most similar to query): '{abstract_i}'\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1. If the current summary is empty, initialize it by summarizing how the new abstract relates to the query abstract.\\n\"\n",
    "            \"2. If the current summary is not empty, update it by integrating any new insights or findings from the new abstract that are relevant to the query abstract.\\n\\n\"\n",
    "            \"Write the updated summary below:\\nSummary:\"\n",
    "        )\n",
    "\n",
    "        # Tokenize prompt and check length\n",
    "        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "        prompt_length = len(prompt_tokens)\n",
    "        \n",
    "        # A simple model might have a smaller max length, so this check is crucial.\n",
    "        model_max_length = model.config.max_position_embeddings\n",
    "        buffer_tokens = 32\n",
    "\n",
    "        total_length = prompt_length + max_new_tokens + buffer_tokens\n",
    "        \n",
    "        if total_length > model_max_length:\n",
    "            excess = total_length - model_max_length\n",
    "            print(f\"⚠️ Prompt too long by {excess} tokens. Trimming current summary.\")\n",
    "            \n",
    "            # Trim current_summary from the head\n",
    "            summary_tokens = tokenizer(current_summary)[\"input_ids\"]\n",
    "            trimmed_summary_tokens = summary_tokens[excess:] if excess < len(summary_tokens) else []\n",
    "            current_summary = tokenizer.decode(trimmed_summary_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Rebuild prompt with trimmed summary\n",
    "            prompt = (\n",
    "                \"You are a scientific summarization assistant. Your task is to update an existing literature summary by integrating new information from a related abstract. \"\n",
    "                \"Your output should be a concise, accurate combined summary that reflects both the original and new findings. Use clear scientific language and avoid redundancy.\\n\\n\"\n",
    "            \n",
    "                \"Here are several examples of how to perform this task:\\n\\n\"\n",
    "            \n",
    "                \"--- Example 1 ---\\n\"\n",
    "                \"Query Abstract: This study investigates the anti-inflammatory effects of drug X-123 in murine models, demonstrating significant reductions in cytokine levels and improved recovery times.\\n\"\n",
    "                \"Current Summary: Drug X-123 reduces inflammation in mice.\\n\"\n",
    "                \"New Abstract: A related study found that X-123 also enhances tissue regeneration in rats by modulating macrophage activity and suppressing pro-inflammatory signaling pathways.\\n\"\n",
    "                \"Combined Summary: Drug X-123 reduces inflammation in mice and enhances tissue regeneration in rats by modulating macrophage activity and suppressing pro-inflammatory signaling pathways.\\n\\n\"\n",
    "            \n",
    "                \"--- Example 2 ---\\n\"\n",
    "                \"Query Abstract: Researchers evaluated compound Y-456 for its effects on cognitive decline in elderly patients, noting improvements in memory retention and executive function over a 12-week trial.\\n\"\n",
    "                \"Current Summary: Compound Y-456 improves cognitive function in elderly patients.\\n\"\n",
    "                \"New Abstract: A follow-up study revealed that Y-456 also reduces oxidative stress in brain tissue and increases synaptic density in the hippocampus.\\n\"\n",
    "                \"Combined Summary: Compound Y-456 improves cognitive function in elderly patients, reduces oxidative stress in brain tissue, and increases synaptic density in the hippocampus.\\n\\n\"\n",
    "            \n",
    "                \"--- Example 3 ---\\n\"\n",
    "                \"Query Abstract: The paper explores the role of protein Z in regulating insulin sensitivity in diabetic mice, showing enhanced glucose tolerance and reduced insulin resistance.\\n\"\n",
    "                \"Current Summary: Protein Z regulates insulin sensitivity in diabetic mice.\\n\"\n",
    "                \"New Abstract: Additional research shows that protein Z also promotes glucose uptake in muscle cells and downregulates inflammatory markers associated with metabolic syndrome.\\n\"\n",
    "                \"Combined Summary: Protein Z regulates insulin sensitivity in diabetic mice, promotes glucose uptake in muscle cells, and downregulates inflammatory markers associated with metabolic syndrome.\\n\\n\"\n",
    "            \n",
    "                \"Now, perform the task with the following inputs:\\n\"\n",
    "                f\"Query Abstract: {query_abstract}\\n\"\n",
    "                f\"Current Summary: {current_summary}\\n\"\n",
    "                f\"New Abstract (ranked {i+1}th most similar to query): {abstract_i}\\n\\n\"\n",
    "                \"Combined Summary:\"\n",
    ")\n",
    "            prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "        \n",
    "        # Final tokenization\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "        # Generate output\n",
    "        # EOS token ID should be the standard tokenizer EOS for non-instruct models\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode and clean output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if generated_text.startswith(prompt):\n",
    "            generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "        # Log for debugging\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        print(\"Prompt token count:\", len(prompt_tokens))\n",
    "        print(\"Generated token count:\", len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "        print(generated_text)\n",
    "        \n",
    "        # Update summary\n",
    "        current_summary = generated_text.strip()\n",
    "\n",
    "    return current_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa44e6df2acf7c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m top_k_abstracts = [docs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_k_indices]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_summary = summarize_literature(\n\u001b[32m      3\u001b[39m     query_abstract =  abstract_text,\n\u001b[32m      4\u001b[39m     top_k_abstracts = top_k_abstracts,\n\u001b[32m      5\u001b[39m     model = model,\n\u001b[32m      6\u001b[39m     tokenizer = tokenizer,\n\u001b[32m      7\u001b[39m     max_new_tokens = \u001b[32m1014\u001b[39m,\n\u001b[32m      8\u001b[39m     temperature = \u001b[32m0.7\u001b[39m,\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36msummarize_literature\u001b[39m\u001b[34m(model, tokenizer, query_abstract, top_k_abstracts, max_new_tokens, temperature, repetition_penalty)\u001b[39m\n\u001b[32m     28\u001b[39m     messages = [\n\u001b[32m     29\u001b[39m         {\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m         }\n\u001b[32m     46\u001b[39m     ]\n\u001b[32m     47\u001b[39m     inputs = tokenizer.apply_chat_template(\n\u001b[32m     48\u001b[39m         messages,\n\u001b[32m     49\u001b[39m         tokenize=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     50\u001b[39m         add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     51\u001b[39m         return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m     ).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     prompt_length = inputs.input_ids.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# PROMPT FOR BASE MODELS (simple text completion)\u001b[39;00m\n\u001b[32m     57\u001b[39m     prompt = (\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are a scientific summarization assistant.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYour task is to update a literature summary based on a new abstract and a query abstract.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWrite the updated summary below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "top_k_abstracts = [docs[i] for i in top_k_indices]\n",
    "final_summary = summarize_literature(\n",
    "    query_abstract =  abstract_text,\n",
    "    top_k_abstracts = top_k_abstracts,\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens = 1014,\n",
    "    temperature = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f03a4-7cec-4644-80ba-bcdc83e2f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694058c1-f607-405c-ae43-8113207751df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watspeed_data_gr_proj",
   "language": "python",
   "name": "watspeed_data_gr_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
